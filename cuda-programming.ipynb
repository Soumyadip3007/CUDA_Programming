{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:19.398503Z","iopub.execute_input":"2025-03-08T19:12:19.398828Z","iopub.status.idle":"2025-03-08T19:12:19.534408Z","shell.execute_reply.started":"2025-03-08T19:12:19.398800Z","shell.execute_reply":"2025-03-08T19:12:19.533335Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:24.696639Z","iopub.execute_input":"2025-03-08T19:12:24.696961Z","iopub.status.idle":"2025-03-08T19:12:24.917322Z","shell.execute_reply.started":"2025-03-08T19:12:24.696934Z","shell.execute_reply":"2025-03-08T19:12:24.916255Z"}},"outputs":[{"name":"stdout","text":"Sat Mar  8 19:12:24 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile hello.cu\n#include<stdio.h>\n\n__global__ void hello_cuda()\n{\n    printf(\"Hello from Cuda Kernel\\n\");\n}\nint main()\n{\n    hello_cuda<<<1,1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:35.033298Z","iopub.execute_input":"2025-03-08T19:12:35.033670Z","iopub.status.idle":"2025-03-08T19:12:35.040089Z","shell.execute_reply.started":"2025-03-08T19:12:35.033636Z","shell.execute_reply":"2025-03-08T19:12:35.039366Z"}},"outputs":[{"name":"stdout","text":"Writing hello.cu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!nvcc hello.cu -o hello\n!./hello","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:39.936323Z","iopub.execute_input":"2025-03-08T19:12:39.936617Z","iopub.status.idle":"2025-03-08T19:12:42.851530Z","shell.execute_reply.started":"2025-03-08T19:12:39.936595Z","shell.execute_reply":"2025-03-08T19:12:42.850271Z"}},"outputs":[{"name":"stdout","text":"Hello from Cuda Kernel\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile hello_multi.cu\n#include<stdio.h>\n\n__global__ void hello_cuda()\n{\n    int threadId=threadIdx.x+blockIdx.x*blockDim.x;\n    printf(\"Hello from thread--> %d \\n\",threadId);\n}\nint main()\n{\n    hello_cuda<<<2,4>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:46.699648Z","iopub.execute_input":"2025-03-08T19:12:46.700014Z","iopub.status.idle":"2025-03-08T19:12:46.705650Z","shell.execute_reply.started":"2025-03-08T19:12:46.699978Z","shell.execute_reply":"2025-03-08T19:12:46.704791Z"}},"outputs":[{"name":"stdout","text":"Writing hello_multi.cu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!nvcc hello_multi.cu -o hello_multi\n!./hello_multi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:52.696027Z","iopub.execute_input":"2025-03-08T19:12:52.696326Z","iopub.status.idle":"2025-03-08T19:12:54.254332Z","shell.execute_reply.started":"2025-03-08T19:12:52.696302Z","shell.execute_reply":"2025-03-08T19:12:54.253436Z"}},"outputs":[{"name":"stdout","text":"Hello from thread--> 0 \nHello from thread--> 1 \nHello from thread--> 2 \nHello from thread--> 3 \nHello from thread--> 4 \nHello from thread--> 5 \nHello from thread--> 6 \nHello from thread--> 7 \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"***Vector Addition in CUDA***","metadata":{}},{"cell_type":"code","source":"%%writefile vector_add.cu\n#include<stdio.h>\n#include<cuda_runtime.h>\n#define N 1000\n__global__ void vectorAdd(int *A, int *B, int *C, int size)\n{\n    int idx=threadIdx.x+blockIdx.x*blockDim.x;\n    if(idx<size)\n    {\n        C[idx]=A[idx]+B[idx];\n    }\n}\nint main()\n{\n    int *h_A,*h_B,*h_C; //Host Vectors on CPU\n    int  *d_A,*d_B,*d_C;\n\n    size_t bytes=N*sizeof(int); //Mem Size\n\n    //Allocate mem on CPU\n    h_A=(int*)malloc(bytes);\n    h_B=(int*)malloc(bytes);\n    h_C=(int*)malloc(bytes);\n    \n    for(int i=0;i<N;i++)\n    {\n       h_A[i]=i;\n       h_B[i]=i*2;\n    }\n    cudaMalloc((void**)&d_A, bytes);  //allocate on GPU\n    cudaMalloc((void**)&d_B, bytes);\n    cudaMalloc((void**)&d_C, bytes);\n\n    cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice); //Copy data from host to device\n    cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice);\n\n    int threadsperblock=256;\n    int blockspergrid=(N+threadsperblock-1)/threadsperblock;\n    vectorAdd<<<blockspergrid,threadsperblock>>>(d_A,d_B,d_C,N);\n\n    cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost);  // Copy result back to host \n\n    printf(\"Sample results: \\n\");\n    for (int i = 0; i < 10; i++) {\n        printf(\"%d + %d = %d\\n\", h_A[i], h_B[i], h_C[i]);\n    }\n    free(h_A);\n    free(h_B);\n    free(h_C);\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:12:59.471905Z","iopub.execute_input":"2025-03-08T19:12:59.472203Z","iopub.status.idle":"2025-03-08T19:12:59.478393Z","shell.execute_reply.started":"2025-03-08T19:12:59.472179Z","shell.execute_reply":"2025-03-08T19:12:59.477552Z"}},"outputs":[{"name":"stdout","text":"Writing vector_add.cu\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!nvcc vector_add.cu -o vector_add\n!./vector_add","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:13:05.047127Z","iopub.execute_input":"2025-03-08T19:13:05.047418Z","iopub.status.idle":"2025-03-08T19:13:06.612240Z","shell.execute_reply.started":"2025-03-08T19:13:05.047396Z","shell.execute_reply":"2025-03-08T19:13:06.611294Z"}},"outputs":[{"name":"stdout","text":"Sample results: \n0 + 0 = 0\n1 + 2 = 3\n2 + 4 = 6\n3 + 6 = 9\n4 + 8 = 12\n5 + 10 = 15\n6 + 12 = 18\n7 + 14 = 21\n8 + 16 = 24\n9 + 18 = 27\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"***Matrix Multiplication using CUDA Kernels***","metadata":{}},{"cell_type":"code","source":"%%writefile matrix_multiplication.cu\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define N 3 \n\n__global__ void matrixMul(int *A, int *B, int *C, int n) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y; \n    int col = blockIdx.x * blockDim.x + threadIdx.x; \n\n    if (row < n && col < n) {\n        int sum = 0;\n        for (int k = 0; k < n; k++) {\n            sum += A[row * n + k] * B[k * n + col]; // Compute dot product\n        }\n        C[row * n + col] = sum;\n    }\n}\n\nint main() {\n    int h_A[N * N], h_B[N * N], h_C[N * N]; // Host matrices\n    int *d_A, *d_B, *d_C; // Device matrices\n\n    size_t bytes = N * N * sizeof(int); // Memory size\n\n    // Initialize matrices A and B\n    printf(\"Matrix A:\\n\");\n    for (int i = 0; i < N * N; i++) {\n        h_A[i] = i + 1;\n        h_B[i] = (i + 1) * 2;\n        printf(\"%d \", h_A[i]);\n        if ((i + 1) % N == 0) printf(\"\\n\");\n    }\n\n    printf(\"\\nMatrix B:\\n\");\n    for (int i = 0; i < N * N; i++) {\n        printf(\"%d \", h_B[i]);\n        if ((i + 1) % N == 0) printf(\"\\n\");\n    }\n\n    // Allocate memory on GPU\n    cudaMalloc((void**)&d_A, bytes);\n    cudaMalloc((void**)&d_B, bytes);\n    cudaMalloc((void**)&d_C, bytes);\n\n    // Copy data from host to device\n    cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice);\n\n    // Define thread and block sizes\n    dim3 threadsPerBlock(16, 16); // 16x16 threads per block\n    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y); // Grid size\n\n    // Launch kernel\n    matrixMul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n\n    // Copy result back to host\n    cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost);\n\n    // Print result\n    printf(\"\\nResult Matrix C:\\n\");\n    for (int i = 0; i < N * N; i++) {\n        printf(\"%d \", h_C[i]);\n        if ((i + 1) % N == 0) printf(\"\\n\");\n    }\n\n    // Free memory\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:19:04.719819Z","iopub.execute_input":"2025-03-08T19:19:04.720181Z","iopub.status.idle":"2025-03-08T19:19:04.726197Z","shell.execute_reply.started":"2025-03-08T19:19:04.720156Z","shell.execute_reply":"2025-03-08T19:19:04.725388Z"}},"outputs":[{"name":"stdout","text":"Writing matrix_multiplication.cu\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!nvcc matrix_multiplication.cu -o matrix_mul\n!./matrix_mul","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:19:23.545057Z","iopub.execute_input":"2025-03-08T19:19:23.545368Z","iopub.status.idle":"2025-03-08T19:19:25.124382Z","shell.execute_reply.started":"2025-03-08T19:19:23.545343Z","shell.execute_reply":"2025-03-08T19:19:25.123581Z"}},"outputs":[{"name":"stdout","text":"Matrix A:\n1 2 3 \n4 5 6 \n7 8 9 \n\nMatrix B:\n2 4 6 \n8 10 12 \n14 16 18 \n\nResult Matrix C:\n60 72 84 \n132 162 192 \n204 252 300 \n","output_type":"stream"}],"execution_count":10}]}